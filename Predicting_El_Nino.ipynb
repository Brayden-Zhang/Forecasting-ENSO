{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "n_O7m0FMwTqC"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is El Nino?\n",
        "\n",
        "\n",
        "\n",
        "*  Cycle of warm/cold temperatures in the equatorial Pacific\n",
        "*   Great impacts on energy/agriculture\n",
        "\n",
        "How do we measure El Nino?\n",
        "\n",
        "* We use the Nino 3.4 Index ( a rolling 3-month average of sea surface temperatures in the equatorial Pacific)\n",
        "\n",
        "Why would we used Neural Nets to forecast NN:\n",
        "* Lighter inference computational cost\n",
        "* We would need to use **simulated climate data** due to lack of historical training data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX-VtxaKvN3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvRJPgHur-MU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.ensemble\n",
        "import scipy.stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Used:\n",
        "\n",
        "\n",
        "\n",
        "*   [Cobe Sea-Surface Temperature Dataset:](https://psl.noaa.gov/data/gridded/data.cobe.html): this is a dataset of historical sea surface temperatures form 1880 to 2018\n",
        "*   [Nino3.4 Indices](https://www.ncdc.noaa.gov/teleconnections/enso/indicators/sst/): The Nino3.4 index measures the 3-month rolling average of equatorial Pacific Ocean temperature anomalies.\n"
      ],
      "metadata": {
        "id": "98jLS491v_Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vX_QrfCGwEhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Variables\n",
        "Sea surface temperature (K)\n",
        "\n",
        "### Output Variables\n",
        "Nino 3.4 Index (indicating the state of ENSO)\n",
        "\n",
        "### Training Set:\n",
        "Training on 1980-1995 sea surface temperature and corresponding Nino 3.4 Index\n",
        "\n",
        "\n",
        "### Val Set:\n",
        "1997-2006 Sea surface temperatures and corresponding Nino 3.4 Index\n",
        "\n",
        "### Test Set:\n",
        "2007-2017 sea surface temperatures and corresponding Nino 3.4 Index\n"
      ],
      "metadata": {
        "id": "oDRqF0w_wHey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n"
      ],
      "metadata": {
        "id": "CuDIjXO6wmTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install netCDF4\n",
        "# !wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.1880to2018.nc\n",
        "# !wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/nino34.long.anom.data.txt"
      ],
      "metadata": {
        "id": "Vn8QUZkYwt_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Nino3.4 index (this index measures the state of ENSO by encoding the average\n",
        "#temperature anomaly in the equatorial Pacific)\n",
        "!gdown --id 1aGvitA8rYrHRDxNd2XD4AAFsahCQsv0t"
      ],
      "metadata": {
        "id": "nw7A3NxEwxAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download sst observations from 1880 to 2018\n",
        "!gdown 1-xefk3imP4Q-8GevIV2YIo82iP5rTdSc"
      ],
      "metadata": {
        "id": "1rf5m2bnwzpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "def load_enso_indices():\n",
        "  \"\"\"\n",
        "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "\n",
        "    pd.Series : monthly ENSO values starting from 1870-01-01\n",
        "  \"\"\"\n",
        "  with open('nino34.long.anom.data.txt') as f:\n",
        "    line = f.readline()\n",
        "    enso_vals = []\n",
        "    while line:\n",
        "        yearly_enso_vals = map(float, line.split()[1:])\n",
        "        enso_vals.extend(yearly_enso_vals)\n",
        "        line = f.readline()\n",
        "\n",
        "  enso_vals = pd.Series(enso_vals)\n",
        "  enso_vals.index = pd.date_range('1870-01-01',freq='MS',\n",
        "                                  periods=len(enso_vals))\n",
        "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
        "  return enso_vals\n",
        "\n",
        "def assemble_basic_predictors_predictands(start_date, end_date, lead_time,\n",
        "                                    use_pca=False, n_components=32):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      start_date        str : the start date from which to extract sst\n",
        "      end_date          str : the end date\n",
        "      lead_time         str : the number of months between each sst\n",
        "                              value and the target Nino3.4 Index\n",
        "      use_pca          bool : whether or not to apply principal components\n",
        "                              analysis to the sst field\n",
        "      n_components      int : the number of components to use for PCA\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of sst temperature anomalies)\n",
        "      and the predictands (np array the ENSO index at the specified lead time).\n",
        "\n",
        "  \"\"\"\n",
        "  ds = xr.open_dataset('sst.mon.mean.trefadj.anom.1880to2018.nc')\n",
        "  sst = ds['sst'].sel(time=slice(start_date, end_date))\n",
        "  num_time_steps = sst.shape[0]\n",
        "\n",
        "  #sst is a 3D array: (time_steps, lat, lon)\n",
        "  #in this tutorial, we will not be using ML models that take\n",
        "  #advantage of the spatial nature of global temperature\n",
        "  #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
        "  #(At each time step, there are lat*lon predictors)\n",
        "  sst = sst.values.reshape(num_time_steps, -1)\n",
        "  sst[np.isnan(sst)] = 0\n",
        "\n",
        "  #Use Principal Components Analysis, also called\n",
        "  #Empirical Orthogonal Functions, to reduce the\n",
        "  #dimensionality of the array\n",
        "  if use_pca:\n",
        "    #TODO: create an option to apply the PCA operation learned\n",
        "    #on the train set to the test set.  Currently, PCA is performed\n",
        "    #independently on the train set and test set.\n",
        "    pca = sklearn.decomposition.PCA(n_components=n_components)\n",
        "    pca.fit(sst)\n",
        "    X = pca.transform(sst)\n",
        "  else:\n",
        "    X = sst\n",
        "\n",
        "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
        "                        pd.DateOffset(months=lead_time)\n",
        "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
        "                      pd.DateOffset(months=lead_time)\n",
        "  y = load_enso_indices()[slice(start_date_plus_lead,\n",
        "                                end_date_plus_lead)]\n",
        "\n",
        "\n",
        "  ds.close()\n",
        "  return X, y\n",
        "\n",
        "def plot_nino_time_series(y, predictions, title):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "    y           pd.Series : time series of the true Nino index\n",
        "    predictions np.array  : time series of the predicted Nino index (same\n",
        "                            length and time as y)\n",
        "    titile                : the title of the plot\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "    None.  Displays the plot\n",
        "  \"\"\"\n",
        "  predictions = pd.Series(predictions, index=y.index)\n",
        "  predictions = predictions.sort_index()\n",
        "  y = y.sort_index()\n",
        "\n",
        "  plt.plot(y, label='Ground Truth')\n",
        "  plt.plot(predictions, '--', label='ML Predictions')\n",
        "  plt.legend(loc='best')\n",
        "  plt.title(title)\n",
        "  plt.ylabel('Nino3.4 Index')\n",
        "  plt.xlabel('Date')\n",
        "  plt.show()\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "FBcG7YYswvcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNCoYHVjxGIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample loading of train, val, and test sets\n",
        "X_train, y_train = assemble_basic_predictors_predictands('1980-01-01','1995-12-31', lead_time=1)\n",
        "X_val, y_val = assemble_basic_predictors_predictands('1997-01-01','2006-12-31', lead_time=1)\n",
        "X_test, y_test = assemble_basic_predictors_predictands('2007-01-01','2017-12-31', lead_time=1)\n"
      ],
      "metadata": {
        "id": "oFscp-Iuw22O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n"
      ],
      "metadata": {
        "id": "n_O7m0FMwTqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's use a linear regression model\n",
        "regr = sklearn.linear_model.LinearRegression()\n",
        "regr.fit(X_train,y_train)\n",
        "\n",
        "predictions = regr.predict(X_val)\n",
        "corr, _ = scipy.stats.pearsonr(predictions, y_val)\n",
        "rmse = mean_squared_error(y_val, predictions)\n",
        "print(\"RMSE: {:.2f}\".format(rmse))\n",
        "\n",
        "plot_nino_time_series(y_val, predictions,\n",
        "    'Predicted and True Nino3.4 Indices on Training Set at 1 Month Lead Time. \\n Corr: {:.2f}'.format(corr))"
      ],
      "metadata": {
        "id": "qJ0NzM7uwMxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of overfitted model\n",
        "X, y = assemble_basic_predictors_predictands('1990-01-01','1995-12-31', 36)\n",
        "regr = sklearn.linear_model.LinearRegression()\n",
        "regr.fit(X,y)\n",
        "\n",
        "predictions = regr.predict(X)\n",
        "corr, _ = scipy.stats.pearsonr(predictions, y)\n",
        "rmse = mean_squared_error(y, predictions)\n",
        "print(\"RMSE: {:.2f}\".format(rmse))\n",
        "\n",
        "plot_nino_time_series(y, predictions, 'Predicted and True Nino3.4 Indices for a 36 Month Lead Time. \\n Corr: {:.2f}'.format(corr))\n"
      ],
      "metadata": {
        "id": "ilSHiuOXQcYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluate the model on a randomly selected test set\"\"\"\n",
        "#X has shape (num_time_steps, lat*lon)\n",
        "#y has shape (num_time_steps)\n",
        "np.random.seed(0)\n",
        "\n",
        "#Randomly shuffle X and y to split them in a train and test set\n",
        "num_time_steps = X.shape[0]\n",
        "permuted_indices = np.random.permutation(np.arange(num_time_steps))\n",
        "\n",
        "#Keep 70% of the data for the test set\n",
        "train_set_proportion = int(0.7 * num_time_steps)\n",
        "X, y = X[permuted_indices], y[permuted_indices]\n",
        "X_train, y_train = X[:train_set_proportion], y[:train_set_proportion]\n",
        "X_test, y_test = X[train_set_proportion:], y[train_set_proportion:]\n",
        "\n",
        "#Instead of writing the above logic yourself, sklearn also provides\n",
        "#a built-in method in the line below.\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "regr = sklearn.linear_model.LinearRegression()\n",
        "regr.fit(X_train,y_train)\n",
        "\n",
        "predictions_test = regr.predict(X_test)\n",
        "corr, _ = scipy.stats.pearsonr(predictions_test, y_test)\n",
        "\n",
        "plot_nino_time_series(y_test, predictions_test,\n",
        "            'Predicted and True Nino3.4 Indices \\n \\\n",
        "            on Test Set at 36 Month Lead Time. Corr: {:.2f}'.format(corr))\n",
        "\n",
        "# can't randomly shuffle train/test set due to data leakage...."
      ],
      "metadata": {
        "id": "lV41SOXrQ7Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# actual unbiased/uncorrelated test set\n",
        "\n",
        "X_train, y_train = assemble_basic_predictors_predictands('1980-01-01','1995-12-31', lead_time=36)\n",
        "X_test, y_test = assemble_basic_predictors_predictands('1997-01-01','2006-12-31', lead_time=36)\n",
        "\n",
        "regr = sklearn.linear_model.LinearRegression()\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "predictions_test = regr.predict(X_test)\n",
        "corr, _ = scipy.stats.pearsonr(predictions_test, y_test)\n",
        "\n",
        "plot_nino_time_series(y_test, predictions_test,\n",
        "            'Predicted and True Nino3.4 Indices \\n \\\n",
        "            on Test Set at 36 Month Lead Time. Corr: {:.2f}'.format(corr))"
      ],
      "metadata": {
        "id": "rw3FAwFlREes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, overfitting occurred while training."
      ],
      "metadata": {
        "id": "5ejpvqXRRkOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "X = 2D matrix of shape $(n, d)$\n",
        "* n = # of time steps\n",
        "* d = # of predictors\n",
        "\n",
        "y = vector of shape (n, 1) that has the target ENSO values\n",
        "w = vector of shape (d,1)\n",
        "\n",
        "In regular least squares linear regression, we are finding the value of W that minimizes:\n",
        "$$\\min || Xw - y||^2$$\n",
        "\n",
        "Using **Ridge Regression**:\n",
        "\n",
        "$$ \\min || Xw-y||^2 + α || w ||^2$$\n",
        "\n",
        "$ α || w ||^2$ is the regularization term... used to penalize the model from learning large weights. As we don't want a small change in one of the inputs to cause a large change in the forecast."
      ],
      "metadata": {
        "id": "tJ8cmqhxRzjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = assemble_basic_predictors_predictands('1990-01-01','2005-12-31',\n",
        "                                                1,use_pca=True)\n",
        "X_test, y_test = assemble_basic_predictors_predictands('2008-01-01','2015-12-31', 1,\n",
        "                                                  use_pca=True)\n",
        "regr_1_month_lr = sklearn.linear_model.LinearRegression()\n",
        "regr_1_month_lr.fit(X_train,y_train)\n",
        "\n",
        "#First, let's plot the performance of the model\n",
        "predictions_test = regr_1_month_lr.predict(X_test)\n",
        "corr, _ = scipy.stats.pearsonr(predictions_test, y_test)\n",
        "plot_nino_time_series(y_test, predictions_test,\n",
        "    'Predicted and True Nino3.4 Indices on Test Set \\\n",
        "    at 1 Month Lead Time. Corr: {:.2f}'.format(corr))\n"
      ],
      "metadata": {
        "id": "ePbHYXNEStQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = assemble_basic_predictors_predictands('1990-01-01','2005-12-31',\n",
        "                                                1,use_pca=True)\n",
        "X_test, y_test = assemble_basic_predictors_predictands('2008-01-01','2015-12-31', 1,\n",
        "                                                  use_pca=True)\n",
        "regr_1_month_ridge = sklearn.linear_model.Ridge(alpha=80000.0)\n",
        "regr_1_month_ridge.fit(X_train,y_train)\n",
        "\n",
        "#First, let's plot the performance of the model\n",
        "predictions_test = regr_1_month_ridge.predict(X_test)\n",
        "corr, _ = scipy.stats.pearsonr(predictions_test, y_test)\n",
        "plot_nino_time_series(y_test, predictions_test,\n",
        "    'Predicted and True Nino3.4 Indices on Test Set \\\n",
        "    at 1 Month Lead Time. Corr: {:.2f}'.format(corr))\n"
      ],
      "metadata": {
        "id": "FzG5L1NPS0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"In this cell, we will visualize the variance of linear regression and ridge regression\"\"\"\n",
        "for experiment_num in range(1000):\n",
        "  perturbed_X_test = X_test * np.random.uniform(-1.05,1.05)\n",
        "\n",
        "  perturbed_predictions_linear = regr_1_month_lr.predict(perturbed_X_test)\n",
        "  perturbed_predictions_linear = pd.Series(perturbed_predictions_linear,\n",
        "                                           index=y_test.index)\n",
        "  plt.plot(perturbed_predictions_linear, '--', label='Linear', color='blue',\n",
        "           alpha=0.6)\n",
        "\n",
        "  perturbed_predictions_ridge = regr_1_month_ridge.predict(perturbed_X_test)\n",
        "  perturbed_predictions_ridge = pd.Series(perturbed_predictions_ridge,\n",
        "                                           index=y_test.index)\n",
        "  plt.plot(perturbed_predictions_ridge, '--', label='Ridge', color='orange',\n",
        "           alpha=0.6)\n",
        "\n",
        "  if experiment_num == 0: plt.legend(loc='best')\n",
        "\n",
        "plt.title(\"Spread of ML Predictions With Perturbed Predictors\")\n",
        "plt.ylabel(\"Nino 3.4 Index\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Cv608UpWbvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"In this cell, we will visualize how the weights of the linear\n",
        "regression model are bigger than those of the ridge regression model\"\"\"\n",
        "\n",
        "#coef_ loads in the coefficients from the trained model\n",
        "regr_1_month_lr_weights = regr_1_month_lr.coef_\n",
        "regr_1_month_ridge_weights = regr_1_month_ridge.coef_\n",
        "\n",
        "plt.bar(range(regr_1_month_lr_weights.shape[0]), regr_1_month_lr_weights,\n",
        "        label='Linear Regression Weights')\n",
        "plt.bar(range(regr_1_month_ridge_weights.shape[0]), regr_1_month_ridge_weights,\n",
        "        label='Ridge Regression Weights')\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('Value of Learned Weight')\n",
        "plt.title('Comparison of the Size of Weights of Linear and Ridge Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IsXpIZ8PWqqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now to determine whether we should use variance scaling:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize a scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform both training and test data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train two models, one with scaled data and one without\n",
        "model_scaled = sklearn.linear_model.Ridge(alpha=80000.0)\n",
        "model_unscaled = sklearn.linear_model.Ridge(alpha=80000.0)\n",
        "\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate performance on the test set\n",
        "predictions_scaled = model_scaled.predict(X_test_scaled)\n",
        "predictions_unscaled = model_unscaled.predict(X_test)\n",
        "\n",
        "corr_scaled, _ = scipy.stats.pearsonr(predictions_scaled, y_test)\n",
        "corr_unscaled, _ = scipy.stats.pearsonr(predictions_unscaled, y_test)\n",
        "\n",
        "print(\"Correlation with scaling: {:.3f}\".format(corr_scaled))\n",
        "print(\"Correlation without scaling: {:.3f}\".format(corr_unscaled))\n",
        "\n",
        "# Decide whether to use variance scaling based on performance\n",
        "if corr_scaled > corr_unscaled:\n",
        "  print(\"Variance scaling improves performance.\")\n",
        "else:\n",
        "  print(\"Variance scaling does not improve performance.\")"
      ],
      "metadata": {
        "id": "sJs6g-mhXAl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Approach\n"
      ],
      "metadata": {
        "id": "dQQJ0ukOX0S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaffold code to load in data.  This code cell is mostly data wrangling\n",
        "\n",
        "\n",
        "def load_enso_indices():\n",
        "  \"\"\"\n",
        "  Reads in the txt data file to output a pandas Series of ENSO vals\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "\n",
        "    pd.Series : monthly ENSO values starting from 1870-01-01\n",
        "  \"\"\"\n",
        "  with open('nino34.long.anom.data.txt') as f:\n",
        "    line = f.readline()\n",
        "    enso_vals = []\n",
        "    while line:\n",
        "        yearly_enso_vals = map(float, line.split()[1:])\n",
        "        enso_vals.extend(yearly_enso_vals)\n",
        "        line = f.readline()\n",
        "\n",
        "  enso_vals = pd.Series(enso_vals)\n",
        "  enso_vals.index = pd.date_range('1870-01-01',freq='MS',\n",
        "                                  periods=len(enso_vals))\n",
        "  enso_vals.index = pd.to_datetime(enso_vals.index)\n",
        "  return enso_vals\n",
        "\n",
        "def assemble_predictors_predictands(start_date, end_date, lead_time,\n",
        "                                    dataset, data_format,\n",
        "                                    num_input_time_steps=1,\n",
        "                                    use_pca=False, n_components=32,\n",
        "                                    lat_slice=None, lon_slice=None):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      start_date           str : the start date from which to extract sst\n",
        "      end_date             str : the end date\n",
        "      lead_time            str : the number of months between each sst\n",
        "                              value and the target Nino3.4 Index\n",
        "      dataset              str : 'observations' 'CNRM' or 'MPI'\n",
        "      data_format          str : 'spatial' or 'flatten'. 'spatial' preserves\n",
        "                                  the lat/lon dimensions and returns an\n",
        "                                  array of shape (num_samples, num_input_time_steps,\n",
        "                                  lat, lon).  'flatten' returns an array of shape\n",
        "                                  (num_samples, num_input_time_steps*lat*lon)\n",
        "      num_input_time_steps int : the number of time steps to use for each\n",
        "                                 predictor sample\n",
        "      use_pca             bool : whether or not to apply principal components\n",
        "                              analysis to the sst field\n",
        "      n_components         int : the number of components to use for PCA\n",
        "      lat_slice           slice: the slice of latitudes to use\n",
        "      lon_slice           slice: the slice of longitudes to use\n",
        "\n",
        "  outputs\n",
        "  -------\n",
        "      Returns a tuple of the predictors (np array of sst temperature anomalies)\n",
        "      and the predictands (np array the ENSO index at the specified lead time).\n",
        "\n",
        "  \"\"\"\n",
        "  file_name = {'observations' : 'sst.mon.mean.trefadj.anom.1880to2018.nc',\n",
        "               'CNRM'         : 'CNRM_tas_anomalies_regridded.nc',\n",
        "               'MPI'          : 'MPI_tas_anomalies_regridded.nc'}[dataset]\n",
        "  variable_name = {'observations' : 'sst',\n",
        "                   'CNRM'         : 'tas',\n",
        "                   'MPI'          : 'tas'}[dataset]\n",
        "  ds = xr.open_dataset(file_name)\n",
        "  sst = ds[variable_name].sel(time=slice(start_date, end_date))\n",
        "  if lat_slice is not None:\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    raise NotImplementedError(\"If you desire, you must implement the slicing!\")\n",
        "  if lon_slice is not None:\n",
        "    \"\"\"YOUR CODE HERE\"\"\"\n",
        "    raise NotImplementedError(\"If you desire, you must implement the slicing!\")\n",
        "\n",
        "\n",
        "  num_samples = sst.shape[0]\n",
        "  #sst is a (num_samples, lat, lon) array\n",
        "  #the line below converts it to (num_samples, num_input_time_steps, lat, lon)\n",
        "  sst = np.stack([sst.values[n-num_input_time_steps:n] for n in range(num_input_time_steps,\n",
        "                                                              num_samples+1)])\n",
        "  #CHALLENGE: CAN YOU IMPLEMENT THE ABOVE LINE WITHOUT A FOR LOOP?\n",
        "  num_samples = sst.shape[0]\n",
        "\n",
        "  sst[np.isnan(sst)] = 0\n",
        "  if data_format=='flatten':\n",
        "    #sst is a 3D array: (time_steps, lat, lon)\n",
        "    #in this tutorial, we will not be using ML models that take\n",
        "    #advantage of the spatial nature of global temperature\n",
        "    #therefore, we reshape sst into a 2D array: (time_steps, lat*lon)\n",
        "    #(At each time step, there are lat*lon predictors)\n",
        "    sst = sst.reshape(num_samples, -1)\n",
        "\n",
        "\n",
        "    #Use Principal Components Analysis, also called\n",
        "    #Empirical Orthogonal Functions, to reduce the\n",
        "    #dimensionality of the array\n",
        "    if use_pca:\n",
        "      pca = sklearn.decomposition.PCA(n_components=n_components)\n",
        "      pca.fit(sst)\n",
        "      X = pca.transform(sst)\n",
        "    else:\n",
        "      X = sst\n",
        "  else: # data_format=='spatial'\n",
        "    X = sst\n",
        "\n",
        "  start_date_plus_lead = pd.to_datetime(start_date) + \\\n",
        "                        pd.DateOffset(months=lead_time+num_input_time_steps-1)\n",
        "  end_date_plus_lead = pd.to_datetime(end_date) + \\\n",
        "                      pd.DateOffset(months=lead_time)\n",
        "  if dataset == 'observations':\n",
        "    y = load_enso_indices()[slice(start_date_plus_lead,\n",
        "                                  end_date_plus_lead)]\n",
        "  else: #the data is from a GCM\n",
        "    X = X.astype(np.float32)\n",
        "    #The Nino3.4 Index is composed of three month rolling values\n",
        "    #Therefore, when calculating the Nino3.4 Index in a GCM\n",
        "    #we have to extract the two months prior to the first target start date\n",
        "    target_start_date_with_2_month = start_date_plus_lead - pd.DateOffset(months=2)\n",
        "    subsetted_ds = ds[variable_name].sel(time=slice(target_start_date_with_2_month,\n",
        "                                                   end_date_plus_lead))\n",
        "    #Calculate the Nino3.4 index\n",
        "    y = subsetted_ds.sel(lat=slice(5,-5), lon=slice(360-170,360-120)).mean(dim=('lat','lon'))\n",
        "\n",
        "    y = pd.Series(y.values).rolling(window=3).mean()[2:].values\n",
        "    y = y.astype(np.float32)\n",
        "  ds.close()\n",
        "  return X.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "\n",
        "class ENSODataset(Dataset):\n",
        "    def __init__(self, predictors, predictands):\n",
        "        self.predictors = predictors\n",
        "        self.predictands = predictands\n",
        "        assert self.predictors.shape[0] == self.predictands.shape[0], \\\n",
        "               \"The number of predictors must equal the number of predictands!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.predictors.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.predictors[idx], self.predictands[idx]"
      ],
      "metadata": {
        "id": "I6hPzsFPX2C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_input_time_steps        (int) : the number of input time\n",
        "                                                steps in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.print_layer = Print()\n",
        "\n",
        "        #TIP: print out the dimension of the extracted features from\n",
        "        #the conv layers for setting the dimension of the linear layer!\n",
        "        #Using the print_layer, we find that the dimensions are\n",
        "        #(batch_size, 16, 42, 87)\n",
        "        self.fc1 = nn.Linear(16 * 42 * 87, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 1)\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        if self.print_feature_dimension:\n",
        "          x = self.print_layer(x)\n",
        "        x = x.view(-1, 16 * 42 * 87)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Print(nn.Module):\n",
        "    \"\"\"\n",
        "    This class prints out the size of the features\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "metadata": {
        "id": "GD04wArSYGHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "pKEd_4nCb8C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(net, criterion, optimizer, trainloader, testloader,\n",
        "                  experiment_name, num_epochs=40):\n",
        "  \"\"\"\n",
        "  inputs\n",
        "  ------\n",
        "\n",
        "      net               (nn.Module)   : the neural network architecture\n",
        "      criterion         (nn)          : the loss function (i.e. root mean squared error)\n",
        "      optimizer         (torch.optim) : the optimizer to use update the neural network\n",
        "                                        architecture to minimize the loss function\n",
        "      trainloader       (torch.utils.data.DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the train dataset\n",
        "      testloader        (torch.utils.data. DataLoader): dataloader that loads the\n",
        "                                        predictors and predictands\n",
        "                                        for the test dataset\n",
        "  outputs\n",
        "  -------\n",
        "      predictions (np.array), and saves the trained neural network as a .pt file\n",
        "  \"\"\"\n",
        "  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  net = net.to(device)\n",
        "  best_loss = np.infty\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    for mode, data_loader in [('train', trainloader), ('test', testloader)]:\n",
        "      #Set the model to train mode to allow its weights to be updated\n",
        "      #while training\n",
        "      if mode == 'train':\n",
        "        net.train()\n",
        "\n",
        "      #Set the model to eval model to prevent its weights from being updated\n",
        "      #while testing\n",
        "      elif mode == 'test':\n",
        "        net.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(data_loader):\n",
        "        # get a mini-batch of predictors and predictands\n",
        "        batch_predictors, batch_predictands = data\n",
        "        batch_predictands = batch_predictands.to(device)\n",
        "        batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #calculate the predictions of the current neural network\n",
        "        predictions = net(batch_predictors).squeeze()\n",
        "\n",
        "        #quantify the quality of the predictions using a\n",
        "        #loss function (aka criterion) that is differentiable\n",
        "        loss = criterion(predictions, batch_predictands)\n",
        "\n",
        "        if mode == 'train':\n",
        "            #the 'backward pass: calculates the gradients of each weight\n",
        "            #of the neural network with respect to the loss\n",
        "            loss.backward()\n",
        "\n",
        "            #the optimizer updates the weights of the neural network\n",
        "            #based on the gradients calculated above and the choice\n",
        "            #of optimization algorithm\n",
        "            optimizer.step()\n",
        "\n",
        "        #Save the model  that have the best performance!\n",
        "        # if mode == 'test' and running_loss < best_loss:\n",
        "            # best_loss = running_loss\n",
        "            # torch.save(net.state_dict(), '{}_best_weights.pt'.format(experiment_name))\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "      if running_loss < best_loss and mode == 'test':\n",
        "          best_loss = running_loss\n",
        "          torch.save(net, '{}.pt'.format(experiment_name))\n",
        "      print('{} Set: Epoch {:02d}. loss: {:3f}'.format(mode, epoch+1, \\\n",
        "                                            running_loss/len(data_loader)))\n",
        "      if mode == 'train':\n",
        "          train_losses.append(running_loss/len(data_loader))\n",
        "      else:\n",
        "          test_losses.append(running_loss/len(data_loader))\n",
        "\n",
        "  net = torch.load('{}.pt'.format(experiment_name))\n",
        "  net.eval()\n",
        "  net.to(device)\n",
        "\n",
        "  #the remainder of this notebook calculates the predictions of the best\n",
        "  #saved model\n",
        "  predictions = np.asarray([])\n",
        "  for i, data in enumerate(testloader):\n",
        "    batch_predictors, batch_predictands = data\n",
        "    batch_predictands = batch_predictands.to(device)\n",
        "    batch_predictors = batch_predictors.to(device)\n",
        "\n",
        "    batch_predictions = net(batch_predictors).squeeze()\n",
        "    #Edge case: if there is 1 item in the batch, batch_predictions becomes a float\n",
        "    #not a Tensor. the if statement below converts it to a Tensor\n",
        "    #so that it is compatible with np.concatenate\n",
        "    if len(batch_predictions.size()) == 0:\n",
        "      batch_predictions = torch.Tensor([batch_predictions])\n",
        "    predictions = np.concatenate([predictions, batch_predictions.detach().cpu().numpy()])\n",
        "  return predictions, train_losses, test_losses\n"
      ],
      "metadata": {
        "id": "4TToSASKZyGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assemble numpy arrays corresponding to predictors and predictands\n",
        "train_start_date = '1960-01-01'\n",
        "train_end_date = '2005-12-31'\n",
        "num_input_time_steps = 2\n",
        "lead_time = 2\n",
        "train_predictors, train_predictands = assemble_predictors_predictands(train_start_date,\n",
        "                      train_end_date, lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
        "test_predictors, test_predictands = assemble_predictors_predictands('2007-01-01',\n",
        "                    '2017-12-31', lead_time, 'observations', 'spatial', num_input_time_steps=num_input_time_steps)\n",
        "\n",
        "#Convert the numpy ararys into ENSODataset, which is a subset of the\n",
        "#torch.utils.data.Dataset class.  This class is compatible with\n",
        "#the torch dataloader, which allows for data loading for a CNN\n",
        "train_dataset = ENSODataset(train_predictors, train_predictands)\n",
        "test_dataset = ENSODataset(test_predictors, test_predictands)\n",
        "\n",
        "#Create a torch.utils.data.DataLoader from the ENSODatasets() created earlier!\n",
        "#the similarity between the name DataLoader and Dataset in the pytorch API is unfortunate...\n",
        "trainloader = DataLoader(train_dataset, batch_size=10)\n",
        "testloader = DataLoader(test_dataset, batch_size=10)\n",
        "net = CNN(num_input_time_steps=num_input_time_steps)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "experiment_name = \"twolayerCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "predictions, train_losses, test_losses = train_network(net, nn.MSELoss(),\n",
        "                  optimizer, trainloader, testloader, experiment_name)"
      ],
      "metadata": {
        "id": "xghNzJxtbe9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NeJKSzyIe_uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Chnx1dm5cUav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "55zMLW9EcYLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing the network architecture\n"
      ],
      "metadata": {
        "id": "ifVdqABGceJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(net)"
      ],
      "metadata": {
        "id": "1WEKn7JAcffi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNReduced(CNN):\n",
        "    def __init__(self, num_input_time_steps=1, print_feature_dimension=False):\n",
        "        \"\"\"\n",
        "        inputs\n",
        "        -------\n",
        "            num_input_time_steps        (int) : the number of input time\n",
        "                                                steps in the predictor\n",
        "            print_feature_dimension    (bool) : whether or not to print\n",
        "                                                out the dimension of the features\n",
        "                                                extracted from the conv layers\n",
        "        \"\"\"\n",
        "        super(CNNReduced, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_input_time_steps, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.print_layer = Print()\n",
        "\n",
        "        #TIP: print out the dimension of the extracted features from\n",
        "        #the conv layers for setting the dimension of the linear layer!\n",
        "        #Using the print_layer, we find that the dimensions are\n",
        "        #(batch_size, 16, 42, 87)\n",
        "        self.fc1 = nn.Linear(16 * 42 * 87, 80)\n",
        "        self.fc2 = nn.Linear(80, 42)\n",
        "        self.fc3 = nn.Linear(42, 1)\n",
        "        self.print_feature_dimension = print_feature_dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        if self.print_feature_dimension:\n",
        "          x = self.print_layer(x)\n",
        "        x = x.view(-1, 16 * 42 * 87)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zdp_LgOfbmJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_reduced = CNNReduced(num_input_time_steps=num_input_time_steps)\n",
        "summary(net_reduced)"
      ],
      "metadata": {
        "id": "-bnZ8C5Jm9Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net_reduced.parameters(), lr=0.0001)\n",
        "\n",
        "experiment_name = \"reducedParametersCNN_{}_{}\".format(train_start_date, train_end_date)\n",
        "predictions, train_losses, test_losses = train_network(net_reduced, nn.MSELoss(),\n",
        "                  optimizer, trainloader, testloader, experiment_name)"
      ],
      "metadata": {
        "id": "XlxLfEHenAlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Performance of {} Neural Network During Training'.format(experiment_name))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEqzQJHhny7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr, _ = pearsonr(test_predictands, predictions)\n",
        "rmse = mean_squared_error(test_predictands, predictions) ** 0.5\n",
        "plot_nino_time_series(\n",
        "    test_predictands,\n",
        "    predictions,\n",
        "    '{} Predictions. Corr: {:3f}. RMSE: {:3f}.'.format(experiment_name, corr, rmse)\n",
        "    )"
      ],
      "metadata": {
        "id": "s24oluUHn1SM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}